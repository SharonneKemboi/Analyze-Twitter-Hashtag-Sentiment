{"cells":[{"source":"# Analyze Twitter Hashtag Sentiment\nSentiment analysis is a powerful tool and can be used to determine whether a given set of text is positive, neutral, or negative in valence. In this template, you will use the Twitter API to access recent tweets using hashtags that you define. You will then compare the sentiment across these different hashtags.\n\nTo be able to use this template, the following criteria must be satisfied:\n- You will need an active Twitter account.\n- You will need a bearer token for accessing the Twitter API. \n\nTo get a bearer token, you will need to navigate to this [page](https://developer.twitter.com/en/portal/petition/essential/basic-info) and sign up for Essential access. This will take you through a short verification process. When you are finished, you should be able to create a new app and generate a bearer token which will be used to access the API.\n\n_Warning: This template will extract real Twitter data. As a result, some content may contain offensive language._","metadata":{},"id":"e9c78c83-1aec-4d06-b80b-2ab703343c27","cell_type":"markdown"},{"source":"## 1. Getting Set Up\nIn order to access the Twitter API, you will need to use an integration to set an environment variable. To add a new integration in your Workspace, click on the Integrations icon in the far left toolbar of the Workspace editing interface. Next, click \"Add Integration\" and \"Environment Variables\". You will need to specify the name (BEARER_TOKEN) and the value (the token you were provided). You can call this \"Twitter Integration\". You can read more about integrations [here](https://workspace-docs.datacamp.com/integrations/environment-variables). Click \"Create\" and follow the remaining steps, and you should be ready to go!\n\nThe code then performs the following:\n1. Installs and imports the packages you will use to retrieve Twitter data and visualize it. \n2. Sets your bearer token for accessing the Twitter API. This does not require further input if you have configured your BEARER_TOKEN environment variable correctly.\n3. Sets the hashtags you want to compare. By default, this template retrieves tweets based on three data science topics. You are free to supply any hashtags you wish to use (a topic preceded by a `#` symbol).\n4. Initializes a tweepy [`Client`](https://docs.tweepy.org/en/stable/client.html). This enables you to make requests to the Twitter API. It will retrieve the last ten tweets for your first hashtag as a test.","metadata":{},"id":"ad409443","cell_type":"markdown"},{"source":"%%capture\n# Install necessary packages\n!pip install tweepy","metadata":{"scrolled":false,"executionTime":4380,"lastSuccessfullyExecutedCode":"%%capture\n# Install necessary packages\n!pip install tweepy"},"id":"6e7846c8","cell_type":"code","execution_count":1,"outputs":[]},{"source":"# Import packages\nimport os\nimport tweepy\nimport pandas as pd\nimport re\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport plotly.express as px\n\n# Set bearer_token for essential access\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n# Define 2-3 hashtags here\nhashtags = [\"#tableau\", \"#python\", \"#powerbi\"]\n\n# Initialize the Tweepy client\nclient = tweepy.Client(bearer_token=bearer_token)\n\n# Confirm the client is initialized by printing the 10 most recent tweets using your hashtag\nfor tweet in client.search_recent_tweets(hashtags[0]).data:\n    print(tweet.text)","metadata":{"scrolled":false,"executionTime":627,"lastSuccessfullyExecutedCode":"# Import packages\nimport os\nimport tweepy\nimport pandas as pd\nimport re\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport plotly.express as px\n\n# Set bearer_token for essential access\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n# Define 2-3 hashtags here\nhashtags = [\"#tableau\", \"#python\", \"#powerbi\"]\n\n# Initialize the Tweepy client\nclient = tweepy.Client(bearer_token=bearer_token)\n\n# Confirm the client is initialized by printing the 10 most recent tweets using your hashtag\nfor tweet in client.search_recent_tweets(hashtags[0]).data:\n    print(tweet.text)","executionCancelledAt":null,"lastExecutedAt":1686553484201,"lastScheduledRunId":null},"id":"placed-thong","cell_type":"code","execution_count":null,"outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import packages\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtweepy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"],"ename":"ModuleNotFoundError","evalue":"No module named 'tweepy'"}]},{"source":"The code above should return the text of the past 10 tweets using the hashtag you supplied. If you have not set up your integration correctly (or are using the wrong bearer token, you may encounter an error such as:\n> Unauthorized: 401 Unauthorized\n\nIf you do encounter such an error, make sure to review the instructions and try again.","metadata":{},"id":"74496e57","cell_type":"markdown"},{"source":"## 2. Create a DataFrame of Tweets\nNext, you can use the client to retrieve a specified number of tweets related to a topic. The code below defines and runs a custom function that uses [`Paginator()`](https://docs.tweepy.org/en/stable/pagination.html?highlight=pagination) to return recent tweets (within the past seven days) that use a specific hashtag. There are two parameters you can customize:\n- The `num_results` you want to return per hashtag. The number must be a multiple of 100, and cannot exceed 2000.\n- The language (`lang`) of the tweets you want to query. This is set to English by default, but you can use other [languages](https://developer.twitter.com/en/docs/twitter-for-websites/supported-languages) if you prefer!\n\nThe code then uses this function to iterate through the list of hashtags you defined and return a DataFrame containing all three result sets.\n\n_Note: Depending on the number of results you return, this code can take some time to execute._","metadata":{},"id":"narrative-desire","cell_type":"markdown"},{"source":"# Define a function to query tweets\ndef get_tweets(hashtag, num_results=1000, lang=\"en\"):\n    # Initialize two empty DataFrames to get user data and tweets\n    tweets_df = pd.DataFrame()\n    \n    # Return the number of batches based on num_results\n    if num_results > 2000:\n        raise ValueError(\"`num_results` must be less than or equal to 2000.\")\n    elif num_results % 100 != 0:\n        raise ValueError(\"`num_results` must be a multiple of 100.\")\n    max_results = 100\n    limit = num_results / max_results\n\n    # Iterate through batches of tweets\n    for tweet_batch in tweepy.Paginator(\n        client.search_recent_tweets,\n        query=hashtag + \" lang:\" + str(lang) + \" -is:retweet\",\n        max_results=100,\n        limit=limit,\n    ):\n        # Retrieve data from batch and add it to DataFrame\n        data = tweet_batch.data\n        batch_data = pd.DataFrame(data)\n        batch_data[\"hashtag\"] = hashtag\n        tweets_df = pd.concat([tweets_df, batch_data])\n\n    # Return DataFrame\n    return tweets_df.reset_index()\n\n\n# Inititialize a DataFrame to store the tweets\nsentiment_df = pd.DataFrame()\n\n# Iterate through the hashtags and add the data\nfor tag in hashtags:\n    temp_df = get_tweets(tag, num_results=1000, lang=\"en\")  # Specify the language here\n    sentiment_df = pd.concat([sentiment_df, temp_df])\n\n# Preview the first DataFrame\nsentiment_df","metadata":{"scrolled":false,"executionCancelledAt":1684847915485},"id":"mathematical-asset","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 3. Process the Text\nThe next step is to perform some light cleaning on the tweets and then perform a sentiment analysis. Two custom functions are defined to fulfill these tasks:\n\n- The first function uses [`re.sub()`](https://docs.python.org/3/library/re.html) to define a regular expression pattern and remove unwanted user mentions and links. \n- The second functions uses the NLTK [`SentimentIntensityAnalyzer()`](https://www.nltk.org/api/nltk.sentiment.vader.html#module-nltk.sentiment.vader) to generate a compound sentiment score for each tweet. This score is an aggregate of negative, neutral, and positive scores and ranges between -1 (very negative) to 1 (very positive).\n\n**Sentiment Analysis Citation**\n>Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.","metadata":{},"id":"mineral-supplement","cell_type":"markdown"},{"source":"# Define function to strip away unwanted characters\ndef clean_tweet(tweet):\n    pattern = \"@\\w+|https.*|\\\\n\"\n    clean_tweet = re.sub(pattern, \" \", tweet)\n    return clean_tweet\n\n# Define function to calculate the compound sentiment score\ndef calculate_sentiment(text):\n    sid = SentimentIntensityAnalyzer()\n    scores = sid.polarity_scores(text)\n    return scores['compound']\n\n# Clean the tweet and store in a new column\nsentiment_df['processed_text'] = sentiment_df['text'].apply(clean_tweet)\n\n# Generate sentiment scores\nsentiment_df['sentiment_score'] = sentiment_df['processed_text'].apply(calculate_sentiment)\n\n# Preview the cleaned and analyzed tweets\nsentiment_df","metadata":{"scrolled":false,"executionCancelledAt":1684847915591},"id":"87da9117","cell_type":"code","execution_count":null,"outputs":[]},{"source":"### 4a. Bar Chart\nA bar chart is a helpful way to to visualize the mean sentiment scores per hashtag. The following code calculates the mean sentiment per hashtag and plots the data in a Plotly [bar chart](https://plotly.com/python/bar-charts/).\n\nYou can interact with the plot by hovering over it to learn the precise mean for each hashtag.","metadata":{},"id":"dried-stadium","cell_type":"markdown"},{"source":"# Aggregate the DataFrame and return the mean sentiment per hashtag\nmovie_means = (\n    sentiment_df.groupby(\"hashtag\")[[\"hashtag\", \"sentiment_score\"]]\n    .mean()\n    .sort_values(by=\"sentiment_score\")\n)\n\n# Create the bar chart\nfig = px.bar(\n    movie_means,\n    x=\"sentiment_score\",\n    y=movie_means.index,\n    labels={\"sentiment_score\": \"Average Sentiment Score\", \"hashtag\": \"Hashtag\"},\n)\n\n# Update the layout and show the figure\nfig.update_layout(\n    template=\"plotly_white\",\n    title_text=\"Average Sentiment Score of Twitter Hashtags\",\n    title_x=0.5,\n    width=800,  # Adjust the width of the plot\n    height=400,  # Adjust the height of the plot\n)\nfig.show()","metadata":{"scrolled":false,"executionCancelledAt":1684847915740},"id":"e8458849","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 4. Visualize the Sentiment Per Hashtag\n### 4b. Strip Chart\nThe next step is to visualize the sentiment scores per hashtag. The following code initializes a [strip chart](https://plotly.com/python/strip-charts/) for each topic using Plotly. \n\nThis creates an interactive visualization that allows you to hover over each point and view the sentiment score and first fifty characters of the tweets (tweets longer than 50 characters are shortened for visibility purposes).\n\nBe sure to examine the points on the upper and lower ends for each hashtag. Do the tweets correspond with the score assigned to them?","metadata":{},"id":"fatty-cooler","cell_type":"markdown"},{"source":"# Process the Tweet text and sentiment scores for easier visualization\nsentiment_df[\"vis_text\"] = sentiment_df[\"processed_text\"].str[:50] + \"...\"\nsentiment_df[\"sentiment_category\"] = pd.cut(\n    sentiment_df[\"sentiment_score\"],\n    bins=[-1, -0.6, -0.2, 0.2, 0.6, 1],\n    labels=[\"Very Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"],\n)\n\n# Create color mapping for sentiment scores\ncolor_map = color_discrete_map = {\n    \"Very Negative\": \"#7d0404\",\n    \"Negative\": \"#b86500\",\n    \"Neutral\": \"#b7a300\",\n    \"Positive\": \"#8db700\",\n    \"Very Positive\": \"#00b54b\",\n}\n\n# Create the box plot\nfig = px.strip(\n    sentiment_df,\n    x=\"hashtag\",\n    y=\"sentiment_score\",\n    color=\"sentiment_category\",\n    labels={  # Assign new labels to the plot\n        \"sentiment_score\": \"Sentiment Score\",\n        \"hashtag\": \"Hashtag\",\n        \"vis_text\": \"Tweet\",\n        \"sentiment_category\": \"Sentiment\",\n    },\n    category_orders={\n        \"sentiment_category\": [\n            \"Very Positive\",\n            \"Positive\",\n            \"Neutral\",\n            \"Negative\",\n            \"Very Negative\",\n        ]\n    },\n    hover_data=[\"vis_text\"],\n    stripmode=\"overlay\",\n    color_discrete_map=color_map,\n)\n\n# Update the layout and show the figure\nfig.update_layout(\n    template=\"plotly_white\",\n    title_text=\"Sentiment Score Distributions Per Hashtag\",\n    title_x=0.5,\n    width=800,  # Adjust the width of the plot\n    height=600,  # Adjust the height of the plot\n)\nfig.show()","metadata":{"scrolled":false,"executionCancelledAt":1684847915889},"id":"lucky-council","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 5. Next Steps\nThis template serves as an introduction to sentiment analysis, but there are many different paths you can take from here. You may want to pursue other forms of social media analysis, predicting sentiment using natural language processing and machine learning, or learn about different ways to visualize data. As a next step, we recommend these DataCamp courses!\n- If you are interested in social media analysis, check out [Analyzing Social Media Data in Python](https://app.datacamp.com/learn/courses/analyzing-social-media-data-in-python). There you can learn more techniques to process and analyze Twitter data. \n- If you are interested in network analysis, we encourage you to look into [Sentiment Analysis in Python](https://app.datacamp.com/learn/courses/sentiment-analysis-in-python). The course mentioned above, Analyzing Social Media Data in Python, also contains content on sentiment analysis.\n- Finally, if you want to learn more about creating beautiful and interactive plots with Plotly, we have a [course](https://app.datacamp.com/learn/courses/introduction-to-data-visualization-with-plotly-in-python) to teach you more ways to create interactive visualizations.","metadata":{},"id":"584bef3d","cell_type":"markdown"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}